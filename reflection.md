# Reflection
To select the datasets for this project, we focused on finding a trustworthy API that can source real time weather data when a user enters a location. For the CSV dataset, we found a  file on Kaggle that includes world capitals and their corresponding weather data, such as current temperature and UV index values. This dataset was perfect for our project because it provided easily accessible and formatted weather info for a large set of cities. This also made it easy for the bot to form formatted responses. For the API data, we used Open-Meteo to get real-time wind speed using latitude and longitude coordinates for the user's string input. Using both these sources, we created a working chatbot that provides accurate responses. We chose these datasets because they are simply formatted and easy to integrate based on the structure of the project and requirements.

One of the biggest challenges during the project was setting up the GCP instance, in my opinion. The actual code for our ETL process and Python chat bot was simpler because of the in-class examples we had for reference. The only time consuming part of that stag was having to normalize the city names between the datasets. But for some reason, working on the GCP took me a lot longer than expected because there was so much to keep track of. Having lots of terminal windows open, pushing and pulling from GitHub, figuring out my Google credits since the academic ones expired, and overall just making sure the app wasn't bug took a lot longer during the GCP stage. In the library, after much trial and error, we had to pause and give ourselves a break because it had gotten to that mind-numbing point where nothing seemed to be fixing it. However, after much needed rest, a fresh and ready set of eyes later helped everything run smoothly and was the key to perservering through the project and onto the Discord implementation.

Throughout the development of this project, we gained a better understanding of how to integrate and manage different data sources within a real, working application. The extra layer of not knowing what your user is going to input makes it important to double check edge cases or find ways to break your own app. One major learning experience was understanding why data cleaning matters because matching user inputs to dataset entries only works when the cases are insensitive. We also discovered how helpful Flask really is and that integrating APIs into a Python code can be done super fast. Working with the API taught us how to handle external requests and also how to plan for incomplete responses. We also made sure the code still worked when running on a Google Cloud VM. Finally, the Discord integration was super cool and showed us how you can use bot code in many different platforms. It also was the most fun step and made us excited to try out other data sources on a Discord bot and see how it reacts to certain questions based on its learned knowledge.

If time allowed, we would have added input for multiple languages, allowing the bot to respond to weather questions about cities in the user's native language. Right now, our data is normalized to English, so it only would work in English-speaking countries or for English-speaking users. We also maybe would have integrated more weather factors from the API, which provided lots of extra data we didn't use, such as current precipitation or humidity. For the actual look of the bot, we could have added an interactive map with city selectors to reduce errors in the user input. It would also make our app seem more polished and professional on the appearance side. Another thing we could have tried to explore with more time is creating accounts so users can save their chat history or put in their preferred language.
